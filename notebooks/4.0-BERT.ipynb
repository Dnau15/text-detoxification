{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertTokenizer, BertForMaskedLM, AutoTokenizer\nimport torch\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-01T19:55:17.168257Z","iopub.execute_input":"2023-11-01T19:55:17.168827Z","iopub.status.idle":"2023-11-01T19:55:17.175381Z","shell.execute_reply.started":"2023-11-01T19:55:17.168780Z","shell.execute_reply":"2023-11-01T19:55:17.174138Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df = pd.read_table('/kaggle/input/paranmt/filtered.tsv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:55:17.181863Z","iopub.execute_input":"2023-11-01T19:55:17.183362Z","iopub.status.idle":"2023-11-01T19:55:19.878368Z","shell.execute_reply.started":"2023-11-01T19:55:17.183314Z","shell.execute_reply":"2023-11-01T19:55:19.877233Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                          reference  \\\n0           0  If Alkar is flooding her with psychic waste, t...   \n1           1                          Now you're getting nasty.   \n2           2           Well, we could spare your life, for one.   \n3           3          Ah! Monkey, you've got to snap out of it.   \n4           4                   I've got orders to put her down.   \n\n                                         translation  similarity  lenght_diff  \\\n0  if Alkar floods her with her mental waste, it ...    0.785171     0.010309   \n1                        you're becoming disgusting.    0.749687     0.071429   \n2                      well, we can spare your life.    0.919051     0.268293   \n3                       monkey, you have to wake up.    0.664333     0.309524   \n4                         I have orders to kill her.    0.726639     0.181818   \n\n    ref_tox   trn_tox  \n0  0.014195  0.981983  \n1  0.065473  0.999039  \n2  0.213313  0.985068  \n3  0.053362  0.994215  \n4  0.009402  0.999348  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>If Alkar is flooding her with psychic waste, t...</td>\n      <td>if Alkar floods her with her mental waste, it ...</td>\n      <td>0.785171</td>\n      <td>0.010309</td>\n      <td>0.014195</td>\n      <td>0.981983</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Now you're getting nasty.</td>\n      <td>you're becoming disgusting.</td>\n      <td>0.749687</td>\n      <td>0.071429</td>\n      <td>0.065473</td>\n      <td>0.999039</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Well, we could spare your life, for one.</td>\n      <td>well, we can spare your life.</td>\n      <td>0.919051</td>\n      <td>0.268293</td>\n      <td>0.213313</td>\n      <td>0.985068</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Ah! Monkey, you've got to snap out of it.</td>\n      <td>monkey, you have to wake up.</td>\n      <td>0.664333</td>\n      <td>0.309524</td>\n      <td>0.053362</td>\n      <td>0.994215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>I've got orders to put her down.</td>\n      <td>I have orders to kill her.</td>\n      <td>0.726639</td>\n      <td>0.181818</td>\n      <td>0.009402</td>\n      <td>0.999348</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:55:19.880454Z","iopub.execute_input":"2023-11-01T19:55:19.880827Z","iopub.status.idle":"2023-11-01T19:55:25.896829Z","shell.execute_reply.started":"2023-11-01T19:55:19.880798Z","shell.execute_reply":"2023-11-01T19:55:25.895659Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f277e2d69a34dacada81e45a3a8b120"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d439f8a6f27a4d6bb9f48e9355acf6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f4d895c72ca40e8af69125a05b1f349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64a18d5f17c14b09ab8053f1b36465a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae6045056b584cdca69f48dd85bbf51d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n\ndef predict_toxicity(texts, device='cpu', clf_name = 's-nlp/roberta_toxicity_classifier_v1'):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    clf = RobertaForSequenceClassification.from_pretrained(clf_name).to(device)\n    clf_tokenizer = RobertaTokenizer.from_pretrained(clf_name)\n    with torch.inference_mode():\n        inputs = clf_tokenizer(texts, return_tensors='pt', padding=True).to(clf.device)\n        out = torch.softmax(clf(**inputs).logits, -1)[:, 1].cpu().numpy()\n    return out\n\n\ndef mask_toxic(sentence, threshold=0.3):\n    words = sentence.split()\n    probabilities = predict_toxicity(words)\n    text_prep = []\n    toxic_indexes = []\n    for _word, _prob in zip(words, probabilities):\n        if _prob > threshold:\n            text_prep.append(\"[MASK]\")\n        else:\n            text_prep.append(_word)\n    text_prep = \" \".join(text_prep)\n    tokenized = tokenizer(text_prep, return_tensors=\"pt\")\n    return tokenized\n\ndef get_mask_index(inputs):\n    mask_token_indexes = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n    return mask_token_indexes","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:55:25.898714Z","iopub.execute_input":"2023-11-01T19:55:25.899221Z","iopub.status.idle":"2023-11-01T19:55:28.184819Z","shell.execute_reply.started":"2023-11-01T19:55:25.899180Z","shell.execute_reply":"2023-11-01T19:55:28.183532Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"def infer(sentence):\n    inputs = mask_toxic(sentence)\n    with torch.no_grad():\n        logits = model(**inputs).logits\n    mask_indexes = get_mask_index(inputs)\n    for mask_token_index in mask_indexes:\n        predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n        inputs.input_ids[0][mask_token_index] = predicted_token_id\n    return inputs","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:55:28.188025Z","iopub.execute_input":"2023-11-01T19:55:28.188453Z","iopub.status.idle":"2023-11-01T19:55:28.196052Z","shell.execute_reply.started":"2023-11-01T19:55:28.188419Z","shell.execute_reply":"2023-11-01T19:55:28.194636Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"inputs = infer(\"I hate idiots and you\")\ntokenizer.decode(inputs.input_ids[0][1:-1])","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:55:28.198183Z","iopub.execute_input":"2023-11-01T19:55:28.198960Z","iopub.status.idle":"2023-11-01T19:55:35.342785Z","shell.execute_reply.started":"2023-11-01T19:55:28.198904Z","shell.execute_reply":"2023-11-01T19:55:35.341316Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/530 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee94362216744ea3bc58852ceed4db97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df94d249e5e849e790393a9434f4a293"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier_v1 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ddd13cc1c06465db96746b29f7034ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71fc01f9caf4d15ad398a9739b554e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6f2bdf8da6b481b9aa4dc04b67a42dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cc76a9c3c184eb393965b244032a92a"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'i hate you and you'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-11-01T19:55:40.144441Z","iopub.execute_input":"2023-11-01T19:55:40.144842Z","iopub.status.idle":"2023-11-01T19:55:40.474668Z","shell.execute_reply.started":"2023-11-01T19:55:40.144811Z","shell.execute_reply":"2023-11-01T19:55:40.473244Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control thegeneration length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"['You are not good']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}