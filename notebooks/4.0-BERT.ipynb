{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import BertTokenizer, BertForMaskedLM, AutoTokenizer\nimport torch\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-05T13:26:15.266626Z","iopub.execute_input":"2023-11-05T13:26:15.267215Z","iopub.status.idle":"2023-11-05T13:26:27.872503Z","shell.execute_reply.started":"2023-11-05T13:26:15.267187Z","shell.execute_reply":"2023-11-05T13:26:27.871085Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate\n!pip install sacrebleu\n!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:26:27.874583Z","iopub.execute_input":"2023-11-05T13:26:27.874963Z","iopub.status.idle":"2023-11-05T13:26:59.188312Z","shell.execute_reply.started":"2023-11-05T13:26:27.874932Z","shell.execute_reply":"2023-11-05T13:26:59.186833Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (9.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting sacrebleu\n  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.6.3)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.23.5)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (4.9.3)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.8.2 sacrebleu-2.3.1\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.23.5)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=9b1f43542707628fc961b5fd804009acefe9a67f08505e9d4b637c558b250dd1\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_table('/kaggle/input/paranmt/filtered.tsv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:26:59.189905Z","iopub.execute_input":"2023-11-05T13:26:59.190198Z","iopub.status.idle":"2023-11-05T13:27:02.393663Z","shell.execute_reply.started":"2023-11-05T13:26:59.190174Z","shell.execute_reply":"2023-11-05T13:27:02.392768Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                          reference  \\\n0           0  If Alkar is flooding her with psychic waste, t...   \n1           1                          Now you're getting nasty.   \n2           2           Well, we could spare your life, for one.   \n3           3          Ah! Monkey, you've got to snap out of it.   \n4           4                   I've got orders to put her down.   \n\n                                         translation  similarity  lenght_diff  \\\n0  if Alkar floods her with her mental waste, it ...    0.785171     0.010309   \n1                        you're becoming disgusting.    0.749687     0.071429   \n2                      well, we can spare your life.    0.919051     0.268293   \n3                       monkey, you have to wake up.    0.664333     0.309524   \n4                         I have orders to kill her.    0.726639     0.181818   \n\n    ref_tox   trn_tox  \n0  0.014195  0.981983  \n1  0.065473  0.999039  \n2  0.213313  0.985068  \n3  0.053362  0.994215  \n4  0.009402  0.999348  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>similarity</th>\n      <th>lenght_diff</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>If Alkar is flooding her with psychic waste, t...</td>\n      <td>if Alkar floods her with her mental waste, it ...</td>\n      <td>0.785171</td>\n      <td>0.010309</td>\n      <td>0.014195</td>\n      <td>0.981983</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Now you're getting nasty.</td>\n      <td>you're becoming disgusting.</td>\n      <td>0.749687</td>\n      <td>0.071429</td>\n      <td>0.065473</td>\n      <td>0.999039</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Well, we could spare your life, for one.</td>\n      <td>well, we can spare your life.</td>\n      <td>0.919051</td>\n      <td>0.268293</td>\n      <td>0.213313</td>\n      <td>0.985068</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Ah! Monkey, you've got to snap out of it.</td>\n      <td>monkey, you have to wake up.</td>\n      <td>0.664333</td>\n      <td>0.309524</td>\n      <td>0.053362</td>\n      <td>0.994215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>I've got orders to put her down.</td>\n      <td>I have orders to kill her.</td>\n      <td>0.726639</td>\n      <td>0.181818</td>\n      <td>0.009402</td>\n      <td>0.999348</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForMaskedLM.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:27:02.396984Z","iopub.execute_input":"2023-11-05T13:27:02.397822Z","iopub.status.idle":"2023-11-05T13:27:07.374431Z","shell.execute_reply.started":"2023-11-05T13:27:02.397788Z","shell.execute_reply":"2023-11-05T13:27:07.373322Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff2fe468c1c54f26bf54c60841d971f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a7b520bfdc94fd5a58957d3c259416d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d3ab7a0a3e248c4a24167b81523c5ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24e65758204846a9aceba916b7f1c26d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08a4a047e09f4446a8d5e421114ec61b"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom transformers import RobertaForSequenceClassification, RobertaTokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n\ndef predict_toxicity(texts, device='cpu', clf_name = 's-nlp/roberta_toxicity_classifier_v1'):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    clf = RobertaForSequenceClassification.from_pretrained(clf_name).to(device)\n    clf_tokenizer = RobertaTokenizer.from_pretrained(clf_name)\n    with torch.inference_mode():\n        inputs = clf_tokenizer(texts, return_tensors='pt', padding=True).to(clf.device)\n        out = torch.softmax(clf(**inputs).logits, -1)[:, 1].cpu().numpy()\n    return out\n\n\ndef mask_toxic(sentence, threshold=0.3):\n    words = sentence.split()\n    probabilities = predict_toxicity(words)\n    text_prep = []\n    toxic_indexes = []\n    for _word, _prob in zip(words, probabilities):\n        if _prob > threshold:\n            text_prep.append(\"[MASK]\")\n        else:\n            text_prep.append(_word)\n    text_prep = \" \".join(text_prep)\n    tokenized = tokenizer(text_prep, return_tensors=\"pt\")\n    return tokenized\n\ndef get_mask_index(inputs):\n    mask_token_indexes = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n    return mask_token_indexes","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:27:07.375744Z","iopub.execute_input":"2023-11-05T13:27:07.376121Z","iopub.status.idle":"2023-11-05T13:27:09.211544Z","shell.execute_reply.started":"2023-11-05T13:27:07.376092Z","shell.execute_reply":"2023-11-05T13:27:09.210214Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"def infer(sentence):\n    inputs = mask_toxic(sentence)\n    with torch.no_grad():\n        logits = model(**inputs).logits\n    mask_indexes = get_mask_index(inputs)\n    for mask_token_index in mask_indexes:\n        predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n        inputs.input_ids[0][mask_token_index] = predicted_token_id\n    decoded_sentence = tokenizer.decode(inputs.input_ids[0][1:-1])\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:27:09.212975Z","iopub.execute_input":"2023-11-05T13:27:09.213233Z","iopub.status.idle":"2023-11-05T13:27:09.218759Z","shell.execute_reply.started":"2023-11-05T13:27:09.213212Z","shell.execute_reply":"2023-11-05T13:27:09.217443Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"inputs = infer(\"I hate idiots and you\")\nprint(inputs)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:27:09.220252Z","iopub.execute_input":"2023-11-05T13:27:09.220515Z","iopub.status.idle":"2023-11-05T13:27:14.592500Z","shell.execute_reply.started":"2023-11-05T13:27:09.220494Z","shell.execute_reply":"2023-11-05T13:27:14.591583Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/530 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1035cccc1d274d138337e9d8bae2866c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0877c4ed7842dd82d814da602b2872"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at s-nlp/roberta_toxicity_classifier_v1 were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5fd3cb25a464d03b09c980ddee62857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090f3968aabf429c8054b75788dc090d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b08f2ff381134a5daf16d224bd191658"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"186dd995867e4fa9b237dc572da8a5b0"}},"metadata":{}},{"name":"stdout","text":"i hate you and you\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\nimport evaluate\n\nsacrebleu_metric = load_metric(\"sacrebleu\")\nrouge_metric = evaluate.load('rouge')\nter_metric = evaluate.load(\"ter\")","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:27:14.593955Z","iopub.execute_input":"2023-11-05T13:27:14.594238Z","iopub.status.idle":"2023-11-05T13:27:19.836487Z","shell.execute_reply.started":"2023-11-05T13:27:14.594211Z","shell.execute_reply":"2023-11-05T13:27:19.835435Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f5df6a080594a90b63777abe7f99f36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f95d243e09fe43149fcc9ecb8791efd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/9.99k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efd6a8556c224347806abe7c820bb01a"}},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import trange\nfrom transformers import logging\n\nlogging.set_verbosity_error()\n\ntoxic_sentences = df.reference[:200].tolist()\ntoxic_sentences_list = [[t] for t in toxic_sentences]\ndetoxified_text = []\nresult = {\n    \"bleu\": 0,\n    \"rouge1\": 0,\n    \"rouge2\": 0,\n    \"TER\": 0\n}\nn = len(toxic_sentences)\nfor i in trange(len(toxic_sentences)):\n    detoxified_sentence = infer(toxic_sentences[i])\n    #words = toxic_sentences[i].split()\n    #toxic_scores = predict_toxicity(words)\n    #detoxified_sentence = \" \".join([sentence for sentence, score in zip(words, toxic_scores) if score < threshold])\n    detoxified_text.append(detoxified_sentence)\n\nresult[\"bleu\"] = sacrebleu_metric.compute(predictions=detoxified_text, references=toxic_sentences_list)[\"score\"]\nrouge_score = rouge_metric.compute(predictions=detoxified_text, references=toxic_sentences_list)\nresult[\"rouge1\"] = rouge_score[\"rouge1\"]\nresult[\"rouge2\"] = rouge_score[\"rouge2\"]\nresult[\"TER\"] = ter_metric.compute(predictions=detoxified_text, references=toxic_sentences_list)[\"score\"]","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:27:19.837714Z","iopub.execute_input":"2023-11-05T13:27:19.838369Z","iopub.status.idle":"2023-11-05T13:32:48.757518Z","shell.execute_reply.started":"2023-11-05T13:27:19.838338Z","shell.execute_reply":"2023-11-05T13:32:48.756574Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 200/200 [05:28<00:00,  1.64s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(result)","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:32:48.760121Z","iopub.execute_input":"2023-11-05T13:32:48.760523Z","iopub.status.idle":"2023-11-05T13:32:48.765058Z","shell.execute_reply.started":"2023-11-05T13:32:48.760501Z","shell.execute_reply":"2023-11-05T13:32:48.763559Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"{'bleu': 67.59722761285171, 'rouge1': 0.9580604262426111, 'rouge2': 0.9075920886922015, 'TER': 12.672701286326824}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-11-05T13:40:38.900831Z","iopub.execute_input":"2023-11-05T13:40:38.901190Z","iopub.status.idle":"2023-11-05T13:40:38.913415Z","shell.execute_reply.started":"2023-11-05T13:40:38.901167Z","shell.execute_reply":"2023-11-05T13:40:38.912203Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"       bleu   rouge1    rouge2       TER\n0  84.32542  0.95095  0.895144  7.657658","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bleu</th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>TER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>84.32542</td>\n      <td>0.95095</td>\n      <td>0.895144</td>\n      <td>7.657658</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}